\documentclass[a4paper, 11pt]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsfonts,amssymb,amsmath,amscd,amsthm,latexsym}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\newcommand{\cst}{\text{cst}}
\newcommand{\Esp}{\mathbb{E}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\lambdat}{\widetilde{\lambda}}
\newcommand{\Xc}{\check{X}}
\newcommand{\Yc}{\check{Y}}
\newcommand{\Yt}{\widetilde{Y}}

\title{Single cell clustering}
\author{AC, EL, TMH, SR}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem and notations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Notations.}
\begin{itemize}
 \item $i = 1 ... n \simeq 1000$ cells, 
 \item $j = 1 ... p \simeq 20000$ genes, 
 \item $Y_{ij} =$ log-expression,
 \item $K$ groups of cells,
 \item $Z_i \in \{1, \dots, K\} =$ cell membership expression.
\end{itemize}

\paragraph{Mixture model.}
\begin{align*}
 Z_i & \sim \Mcal(1; \pi) \\
 Y_{ij} \mid Z_i = k & \sim \Ncal(\mu + \alpha_i + \beta_j + \gamma_{kj}, \sigma_k^2)
\end{align*}
where $\gamma_{kj}$ is block-wise constant along the locations of the genes: proxy for the mean genome copy number at the location of gene $j$ in cluster $k$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference via fused-lasso}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Penalized likelihood.}
We aim at maximizing
$$
\log p_\theta(Y) - n \lambda \sum_k \pi_k \sum_{j\geq 2} |\gamma_{k,j} - \gamma_{k, j-1}|,
$$
which amounts at maximizing
\begin{align*}
 & \Esp \left(\log p_\theta(Y, Z)  \mid Y\right)- \lambda \sum_k \sum_{j\geq 2} |\gamma_{k,j} - \gamma_{k, j-1}| \\
 = & \sum_{i, k} \tau_{ik} \left(\log \pi_k - \frac12 \sum_j \left( \log\sigma_k^2 - \frac1{\sigma_k^2} (Y_{ij} - \mu - \alpha_i - \beta_j - \gamma_{kj})^2 \right) \right) \\
 & \quad - n \lambda \sum_k \pi_k \sum_{j\geq 2} |\gamma_{k,j} - \gamma_{k, j-1}| + \cst
\end{align*}

\paragraph{Estimating the $\gamma_{kj}$} amounts at minimizing (all other parameters being known),
\begin{align*}
 & \sum_i \tau_{ik} \sum_j (\Yt_{ij} -\gamma_{kj})^2  + 2 n \pi_k \lambda \sigma_k^2 \sum_{j\geq 2} |\gamma_{k,j} - \gamma_{k, j-1}| \\
 = & \sum_i \tau_{ik} \|\Yt_i -\gamma_k\|^2  + 2 n \pi_k \lambda \sigma_k^2 \sum_{j\geq 2} |\gamma_{k,j} - \gamma_{k, j-1}| \\
 = & \sum_i \tau_{ik} \|\Yt_i - X_i \delta_k\|^2  + 2 n \pi_k \lambda \sigma_k^2 \sum_{j\geq 2} |\delta_{k,j}| \\
 = & \sum_i \|\Yc_i - \Xc_i \delta_k\|^2  + 2 n \pi_k \lambda \sigma_k^2 \sum_{j\geq 2} |\delta_{k,j}| \\ 
 = & \|\Yc - \Xc \delta_k\|^2  + \lambdat_k \sum_{j\geq 2} |\delta_{k,j}| 
\end{align*}
where 
\begin{itemize}
 \item $\Yt_{ij} = Y_{ij} - \mu - \alpha_i - \beta_j$,
 \item $X_i$ is the $p \times p$ lower-triagular matrix filled with ones (the same for all $i$), 
 \item $\delta_{kj} = \gamma_{k,j} - \gamma_{k, j-1}$, 
 \item $\Yc_i = \text{diag}([\sqrt{\tau_{ik}}]_j) \Yt_i$, $\Xc_i = \text{diag}([\sqrt{\tau_{ik}}]_j) X_i$,
 \item $\Yc$ (resp $\Xc$) is obtained by piling up the $\Yc_i$ (reps $X_i$).
\end{itemize}
This optimization can be carried, for each $k$ using, e.g., {\tt glmnet}, but $\Yc$ is $np \times 1$ and $\Xc$ is $np \times (n+p)$...

\paragraph{Remarks}
\begin{itemize}
 \item How to tune $\lambda$ ? How does it combine with EM?
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference via dynamic programming}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Penalized likelihood.}
We aim at maximizing
$$
\log p_\theta(Y) - \text{pen}(\{t_{kq}\}),
$$
where $t_{kq}$ is the $q$th change-point in group $k$, which amounts at maximizing
\begin{align*}
 & \Esp \left(\log p_\theta(Y, Z)  \mid Y\right) - \text{pen}(\{t_{kq}\}).
\end{align*}
Remind that the Zhang \& Siegmund penalty can be combined with the segments cost.

\paragraph{Estimating the $\gamma_{kj}$.} Denote $I_{kq} = [t_{k, -1}+1, t_{kq}]$ the $\ell$th segment in cells from group $k$ and $Q_k$ the number of segments, we have that
\begin{align*}
  \log p_\theta(Y, Z)
  = & \log p(Z) + \log p(Y \mid Z) \\
  = & \sum_{i, k} Z_{ik} \left(\log \pi_k - \frac{p}2 \log \sigma_k^2 - \frac1{2\sigma_k^2} \sum_{q=1}^{Q_k} \sum_{j \in I_{kq}} (Y_{ij} - \mu - \alpha_i - \beta_j - \gamma_{kq})^2 \right) + \cst
\end{align*}
so, denoting $\tau_{ik} := \Esp(Z_{ik} \mid Y)$ and $N_k = \sum_{i} \tau_{ik}$, we get (up to an additive constant)
\begin{align*}
  \Esp( \log p_\theta(Y, Z) \mid Y )
  = & \sum_k N_k \log \pi_k 
  - \frac{p}2 \sum_k N_k \log \sigma_k^2 \\
  & - \sum_k \frac1{2\sigma_k^2} \sum_{q=1}^{Q_k} \sum_{j \in I_{kq}} \underset{C_{qk}(\gamma_{kq})}{\underbrace{\sum_i \tau_{ik} (Y_{ij} - \mu - \alpha_i - \beta_j - \gamma_{kq})^2}}. 
\end{align*}
For given $\mu$, $\alpha_i$ and $\beta_j$, the minimizer of $C_{qk}(\gamma_{kq})$
is
$$
\widehat{\gamma}_{kq} = \sum_{j \in I_{kq}} \sum_i \tau_{ik} (Y_{ij} - \mu - \alpha_i - \beta_j)
\left/ \sum_{j \in I_{kq}} \sum_i \tau_{ik}  \right.
$$


\end{document}
